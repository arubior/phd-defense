<section id="embedding"
	 data-background-video="videos/chapter-blue.mp4"
         data-background-video-loop data-background-video-muted>
	<h1><p class="fragment fade-in"; style="color:white;">FASHION MULTI-MODAL EMBEDDING</p></h1>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Introduction</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Motivation</h3>
<div class="container">
  <video data-autoplay loop src="chapters/embedding/videos/instagram.mp4"; height=400px></video>
  <div class="container__text">
<ul>
<li>Increasing online access and smartphone usage</li>
<li>Images uploaded by the users have a lot of information </li>
<li>Images not tagged or described are virtually unfindable by classical search</li>
</ul>
  </div>
</div>
<aside class="notes">
Finding a product -> daunting task<br>
Thousands of images + metadata, needle in a haystack<br>
(Instagram users upload every day 95 million pictures and 500 million stories)<br>
Some numbers<br>
Machine learning techniques are required<br>
Specific system needed to narrow the gap between img and text descriptors<br>
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Motivation</h3>
<ul>
<li>Multi-modal data: <span class="fragment highlight-current-blue"><b>images</b></span> with <span class="fragment highlight-current-blue"><b>textual metadata</b></span>.
</li>
</ul>
</section>

<section data-background-iframe="https://www.tylerstx.com/?_ga=2.115307919.961234722.1625700494-1019772797.1625700494"
          data-background-interactive>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Goals</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/embedding/embedding1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/embedding/embedding2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/embedding/embedding3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/embedding/embedding4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/embedding/embedding5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/embedding/embedding6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/embedding/embedding7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Contributions</h3>
<ul>
<!-- li>NN architecture for processing images and text trained such that dist between the output of related img-text is minimized and unrelated maximized</li-->
<li>Representation space (<span class="fragment highlight-current-blue">embedding</span>) for features coming
from <span class="fragment highlight-current-blue">visual</span> and <span class="fragment highlight-current-blue">textual</span> data</li>
<li>Learned features:</li>
<ul>
<li>are <span class="fragment highlight-current-blue">smooth</span></li>
<li>come from <span class="fragment highlight-current-blue">multiple explanatory features</span></li>
<li>retain <span class="fragment highlight-current-blue">semantic information</span>, therefore are valid for retrieval and classification tasks</li>
</ul>
<li>Textual representations with the <span class="fragment highlight-current-blue">continuity</span> of the visual space</li>
<li><span class="fragment highlight-current-blue">Evaluation</span> of retrieval and classification tasks</li>
</ul>
<aside class="notes">
smooth features: small changes in input -> small changes in embedding
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Context</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/soa/soa1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/soa/soa2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/soa/soa3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/soa/soa4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/soa/soa5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/soa/soa6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/soa/soa7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Related work</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Multi-modal embeddings</h3>
<img data-src="chapters/embedding/imgs/soa/emb2-01.png"; height="175px">
<img data-src="chapters/embedding/imgs/soa/emb1-01.png"; height="175px">
<ul>
<li>Creation of common spaces for vectorial representations of images and texts</li>
<li>Normally using semantical tags, <span class="fragment highlight-current-blue"><b>not rich textual descriptions</b></span> like we do</li>
<li>Our embedding is trained using <span class="fragment highlight-current-blue"><b>real-world challenging</b></span> fashion data</li>
</ul>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Retrieval</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/cbir1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/cbir1.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/cbir2.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/cbir3.png">
</div>
<aside class="notes">
Still, bridging the so-called "semantic gap" between low-level pixel features readable by machines and high-level semantic information understood by humans remains one of the main problems of CBIR
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Method</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Network architecture</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/network/network1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/network/network2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/network/network3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/network/network4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/network/network5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/network/network6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/network/network7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Network architecture</h3>
<ul>
<li><b>Image branch</b></li>
<ul>
<li>AlexNet architecture pretrained on a fashion subset of ImageNet</li>
<li>Last layer replaced by a 128-dimensional output FC</li>
</ul>
<li><b>Text branch</b></li>
<ul>
<li>500-dimensional word2vec descriptors</li>
<li>Context window size of 3 words</li>
<li>Trained with >400k fashion textual descriptions</li>
</ul>
</ul>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>word2vec</h3>
<img data-src="chapters/embedding/imgs/word2vec_scheme1.png"; height="600">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>word2vec</h3>
Text preprocessing
<img data-src="chapters/embedding/imgs/fitter/fitter1-01.png">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>word2vec</h3>
<img data-src="chapters/embedding/imgs/shirt.png">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Contrastive loss</h3>
<video data-autoplay src="videos/contrastive.mp4"></video>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png"
>
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive0.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive1.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive2.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive3.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive4.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Contrastive loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/contrastive5.png">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Global loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/losses1.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
<h3>Global loss</h3>
<img data-src="chapters/embedding/imgs/contrastive/losses2.png">
</section>
<section data-background-image="imgs/backgrounds/blue-01.png"
         >
         id="globalloss">
<h3>Global loss</h3>
<a data-preview-link href="#/overlaploss">
<img data-src="chapters/embedding/imgs/contrastive/losses3.png"></a>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Dataset</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<ul>
<li><span class="fragment highlight-current-blue"><em>431,841 images</em></span>
of products with
<span class="fragment highlight-current-blue"><em>associated texts</em></span>.<br></li>
<li>Textual information separated in the following fields:<br>
<span class="fragment highlight-current-blue"><em>title</em></span>,
<span class="fragment highlight-current-blue"><em>description</em></span>,
<span class="fragment highlight-current-blue"><em>gender</em></span>,
<span class="fragment highlight-current-blue"><em>type</em></span>,
<span class="fragment highlight-current-blue"><em>color</em></span>, and
<span class="fragment highlight-current-blue"><em>category</em></span>.</li>
<ul>
<img data-src="chapters/embedding/imgs/klav.png"; height="250px">
<img data-src="chapters/embedding/imgs/pants.png"; height="250px">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Dataset categories</h3>
<small>
<table>
<tbody>
<tr>
<td>vest</td>
<td>hat</td>
<td>boots</td>
<td>polo</td>
</tr>
<tr>
<td>jewelry</td>
<td>skirt</td>
<td>clutch/wallet</td>
<td>cardigan</td>
</tr>
<tr>
<td>shirt</td>
<td>dress</td>
<td>backpack</td>
<td>swimwear</td>
</tr>
<tr>
<td>suits</td>
<td>travel</td>
<td>bags</td>
<td>glasses/sunglasses</td>
</tr>
<tr>
<td>pants/leggings</td>
<td>flats</td>
<td>shorts</td>
<td>coat/cape</td>
</tr>
<tr>
<td>tops</td>
<td>pump/wedge</td>
<td>sweatshirt/hoodie</td>
<td>blazer</td>
</tr>
<tr>
<td>top</td>
<td>handles</td>
<td>belts</td>
<td>jacket</td>
</tr>
<tr>
<td>other</td>
<td>accessories</td>
<td>jumpsuits</td>
<td>sweater</td>
</tr>
<tr>
<td>joggers</td>
<td>sandals</td>
<td>crossbody/messenger</td>
<td>bag</td>
</tr>
</tbody>
</table>
</small>
<aside class="notes">
32 categories
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Results</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<ul>
<li>1st approach: Bag of Words: based on frequency, ignoring grammar or context. Also, vectors have 4k/8k dimensions depending on the training set.</li>
<li>2nd approach: word2vec: shallow neural networks trained to reconstruct context. Vectors are distributed representations with a fixed dimension, independently on the training set.</li>
</ul>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Retrieval results</h2>
<aside class="notes">
1st approach: Bag of Words: based on frequency, ignoring grammar or context. Also, vectors have 4k/8k dimensions depending on the training set.<br>
2nd approach: word2vec: shallow neural networks trained to reconstruct context. Vectors are distributed representations with a fixed dimension, independently on the training set.<br>
</aside>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin-left:auto;margin-right:auto}
.tg td{border-color:black;border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:34px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:34px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-18eh{border-color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-wp8o{border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-mqa1{border-color:#000000;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-18eh" rowspan="2">Model</th>
    <th class="tg-18eh" colspan="2">Median rank</th>
  </tr>
  <tr>
    <td class="tg-mqa1">Img. vs. txt</td>
    <td class="tg-mqa1">Txt vs. img.</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix">KCCA</td>
    <td class="tg-wp8o">52.42%</td>
    <td class="tg-wp8o">46.65%</td>
  </tr>
  <tr>
    <td class="tg-nrix">Bag of Words</td>
    <td class="tg-wp8o">4.50%</td>
    <td class="tg-wp8o">4.54%</td>
  </tr>
  <tr>
    <td class="tg-nrix">word2vec</td>
    <td class="tg-mqa1">1.61%</td>
    <td class="tg-mqa1">1.63%</td>
  </tr>
</tbody>
</table>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Retrieval results</h2>
<table class="tg">
<thead>
  <tr>
    <th class="tg-18eh" rowspan="2">Model</th>
    <th class="tg-18eh" colspan="2">Image</th>
    <th class="tg-18eh" colspan="2">Text</th>
  </tr>
  <tr>
    <td class="tg-nrix"><span style="font-weight:700">f@5%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@10%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@5%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@10%</span></td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix">KCCA</td>
    <td class="tg-wp8o">3.70</td>
    <td class="tg-wp8o">7.59</td>
    <td class="tg-wp8o">3.90</td>
    <td class="tg-wp8o">9.59</td>
  </tr>
  <tr>
    <td class="tg-baqh">Bag of Words</td>
    <td class="tg-wp8o">53.18</td>
    <td class="tg-wp8o">75.02</td>
    <td class="tg-wp8o">53.14</td>
    <td class="tg-wp8o">74.20</td>
  </tr>
  <tr>
    <td class="tg-baqh">word2vec</td>
    <td class="tg-mqa1">77.90</td>
    <td class="tg-mqa1">89.24</td>
    <td class="tg-mqa1">77.47</td>
    <td class="tg-mqa1">89.78</td>
  </tr>
</tbody>
</table>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Classification results</h2>
<table class="tg">
<thead>
  <tr>
    <th class="tg-18eh" rowspan="2">Model</th>
    <th class="tg-18eh" colspan="2">Classification accuracy</th>
  </tr>
  <tr>
    <td class="tg-18eh">Text</td>
    <td class="tg-18eh">Image</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Bag of Words</td>
    <td class="tg-wp8o">99.78%</td>
    <td class="tg-wp8o">71.73%</td>
  </tr>
  <tr>
    <td class="tg-0lax">word2vec</td>
    <td class="tg-mqa1">99.97%</td>
    <td class="tg-mqa1">90.06%</td>
  </tr>
</tbody>
</table>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h4>Query</h4>
<small>ELEVENTY, piquet, solid color, polo collar, long sleeves, no appliqués, no pockets, small sized. 100% Cotton.</small>
<br>
<br>
<h4>Closest images</h4>
<img data-src="chapters/embedding/imgs/results.png">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Summary</h2>
<ul>
<li>Presented retrieval results on a <span class="fragment highlight-current-blue">challenging real-world dataset</span>, obtaining low median rank values</li>
<li>At the same time, <span class="fragment highlight-current-blue">very good classification accuracy</span> for both images and texts</li>
<li>Approach <span class="fragment highlight-current-blue">easily amenable</span> to large existing e-commerce datasets</li>
<li>Opportunity for e-commerces to <span class="fragment highlight-current-blue">classify and tag</span> images without associated text or fill in missing information looking for similar texts</li>
</ul>
<aside class="notes">
Training the embedding such that distances correspond to similarities, our approach can be easily used for retrieval tasks.<br>
Furthermore, our auxiliary classification networks encourage the embedding to have semantic meaning, making it suitable as features for classification tasks.<br>
</aside class="notes">
</section>