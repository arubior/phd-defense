<section id="embedding"
	 data-background-video="videos/chapters/chapter-blue.mp4"
         data-background-video-loop data-background-video-muted>
	<h1><p class="fragment fade-in"; style="color:white;">FASHION MULTI-MODAL EMBEDDING</p></h1>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Introduction</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Motivation</h3>
<div class="container">
  <video data-autoplay loop src="chapters/embedding/videos/instagram.mp4"; height=400px></video>
  <div class="container__text">
<ul>
<li>Increasing online access and smartphone usage</li>
<li>Images uploaded by the users have a lot of information </li>
<li>Images not tagged or described are virtually unfindable by classical search</li>
</ul>
  </div>
</div>
<aside class="notes">
Finding a product -> daunting task<br>
Thousands of images + metadata, needle in a haystack<br>
(Instagram users upload every day 95 million pictures and 500 million stories)<br>
Some numbers<br>
Machine learning techniques are required<br>
Specific system needed to narrow the gap between img and text descriptors<br>
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Motivation</h3>
<ul>
<li>Multi-modal data: <span class="fragment highlight-current-blue"><b>images</b></span> with <span class="fragment highlight-current-blue"><b>textual metadata</b></span>.
</li>
</ul>
</section>

<section data-background-iframe="https://www.tylerstx.com/?_ga=2.115307919.961234722.1625700494-1019772797.1625700494"
          data-background-interactive>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Goals</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/embedding/embedding1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/embedding/embedding2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/embedding/embedding3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/embedding/embedding4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/embedding/embedding5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/embedding/embedding6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/embedding/embedding7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Contributions</h3>
<ul>
<!-- li>NN architecture for processing images and text trained such that dist between the output of related img-text is minimized and unrelated maximized</li-->
<li>Representation space (<span class="fragment highlight-current-blue">embedding</span>) for features coming
from <span class="fragment highlight-current-blue">visual</span> and <span class="fragment highlight-current-blue">textual</span> data</li>
<li>Learned features:</li>
<ul>
<li>are <span class="fragment highlight-current-blue">smooth</span></li>
<li>come from <span class="fragment highlight-current-blue">multiple explanatory features</span></li>
<li>retain <span class="fragment highlight-current-blue">semantic information</span>, therefore are valid for retrieval and classification tasks</li>
</ul>
<li>Textual representations with the <span class="fragment highlight-current-blue">continuity</span> of the visual space</li>
<li><span class="fragment highlight-current-blue">Evaluation</span> of retrieval and classification tasks</li>
</ul>
<aside class="notes">
smooth features: small changes in input -> small changes in embedding
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Context</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/soa/soa1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/soa/soa2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/soa/soa3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/soa/soa4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/soa/soa5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/soa/soa6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/soa/soa7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Related work</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Multi-modal embeddings</h3>
<img data-src="chapters/embedding/imgs/soa/emb2-01.png"; height="175px">
<img data-src="chapters/embedding/imgs/soa/emb1-01.png"; height="175px">
<ul>
<li>Creation of common spaces for vectorial representations of images and texts</li>
<li>Normally using semantical tags, <span class="fragment highlight-current-blue"><b>not rich textual descriptions</b></span> like we do</li>
<li>Our embedding is trained using <span class="fragment highlight-current-blue"><b>real-world challenging</b></span> fashion data</li>
</ul>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Retrieval</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/cbir1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/cbir2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/cbir3.png">
</div>
<aside class="notes">
Still, bridging the so-called "semantic gap" between low-level pixel features readable by machines and high-level semantic information understood by humans remains one of the main problems of CBIR
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Method</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Network architecture</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/network/network1.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/network/network2.png">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/network/network3.png">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/network/network4.png">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/network/network5.png">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/network/network6.png">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/network/network7.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Network architecture</h3>
<ul>
<li><b>Image branch</b></li>
<ul>
<li>AlexNet architecture pretrained on a fashion subset of ImageNet</li>
<li>Last layer replaced by a 128-dimensional output FC</li>
</ul>
<li><b>Text branch</b></li>
<ul>
<li>500-dimensional word2vec descriptors</li>
<li>Context window size of 3 words</li>
<li>Trained with >400k fashion textual descriptions</li>
</ul>
</ul>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>word2vec</h3>
<img data-src="chapters/embedding/imgs/word2vec_scheme1.png"; height="600">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Text preprocessing</h3>
Removing unwanted symbols, stop words and inflections.
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/fitter/fitter-01.png">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/fitter/fitter-02.png">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>word2vec result example</h3>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-1wig">Query</th>
    <th class="tg-1wig">Similar words</th>
    <th class="tg-1wig">Score</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1" rowspan="6">shirt</td>
    <td class="tg-0lax">tee</td>
    <td class="tg-0lax">0.872</td>
  </tr>
  <tr>
    <td class="tg-0lax">tshirt</td>
    <td class="tg-0lax">0.820</td>
  </tr>
  <tr>
    <td class="tg-0lax">tank</td>
    <td class="tg-0lax">0.761</td>
  </tr>
  <tr>
    <td class="tg-0lax">onesie</td>
    <td class="tg-0lax">0.699</td>
  </tr>
  <tr>
    <td class="tg-0lax">sweatshirt</td>
    <td class="tg-0lax">0.695</td>
  </tr>
  <tr>
    <td class="tg-0lax">longsleeve</td>
    <td class="tg-cly1">0.694</td>
  </tr>
</tbody>
</table>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Contrastive loss</h3>
  <div class="r-stack">
  <img class="fragment fade-out" data-fragment-index="0" src="chapters/embedding/imgs/pairs/pairs1.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="0" src="chapters/embedding/imgs/pairs/pairs2.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="1" src="chapters/embedding/imgs/pairs/pairs3.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="2" src="chapters/embedding/imgs/pairs/pairs4.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="3" src="chapters/embedding/imgs/pairs/pairs5.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="4" src="chapters/embedding/imgs/pairs/pairs6.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="5" src="chapters/embedding/imgs/pairs/pairs7.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="6" src="chapters/embedding/imgs/pairs/pairs8.png"; height="500">
  <img class="fragment current-visible" data-fragment-index="7" src="chapters/embedding/imgs/pairs/pairs9.png"; height="500">
</div>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Contrastive loss</h3>
<video data-autoplay src="chapters/embedding/videos/contrastive.mp4"></video>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Dataset</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<ul>
<li><span class="fragment highlight-current-blue"><em>431,841 images</em></span>
of products with
<span class="fragment highlight-current-blue"><em>associated texts</em></span>.<br></li>
<li>Textual information separated in the following fields:<br>
<span class="fragment highlight-current-blue"><em>title</em></span>,
<span class="fragment highlight-current-blue"><em>description</em></span>,
<span class="fragment highlight-current-blue"><em>gender</em></span>,
<span class="fragment highlight-current-blue"><em>type</em></span>,
<span class="fragment highlight-current-blue"><em>color</em></span>, and
<span class="fragment highlight-current-blue"><em>category</em></span>.</li>
<ul>
<img data-src="chapters/embedding/imgs/klav.png"; height="250px">
<img data-src="chapters/embedding/imgs/pants.png"; height="250px">
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h3>Dataset categories</h3>
<p><b>32 garment categories:</b> <em>shirt</em>, <em>hat</em>, <em>polo</em>, <em>skirt</em>, <em>jewelry</em>, ...</p>
<img data-src="chapters/embedding/imgs/categories/categories-01.png"; width=auto; height=310px>
<img data-src="chapters/embedding/imgs/categories/categories-02.png"; width=auto; height=310px>
<img data-src="chapters/embedding/imgs/categories/categories-03.png"; width=auto; height=310px>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Results</h2>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h4>Query</h4>
<small>
<span class="fragment highlight-current-blue" data-fragment-index="0">Solid color</span>,
<span class="fragment highlight-current-blue" data-fragment-index="1">polo collar, </span>
<span class="fragment highlight-current-blue" data-fragment-index="2">long sleeves, </span>
<span class="fragment highlight-current-blue" data-fragment-index="3">no pockets, </span>
<span class="fragment highlight-current-blue" data-fragment-index="4">small sized. </span>
<span class="fragment highlight-current-blue" data-fragment-index="5">100% Cotton</span>
</small>
<p class="fragment current-visible" data-fragment-index="6">Can you find the closest image?</p>
<h4 class="fragment fade-in" data-fragment-index="7">Closest images</h4>
<div class="r-stack">
<img class="fragment current-visible" data-fragment-index="7" data-src="chapters/embedding/imgs/results.png"; height="350">
<img class="fragment current-visible" data-fragment-index="8" data-src="chapters/embedding/imgs/results-gt.png"; height="350">
</div>
</section>


<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Retrieval results</h2>
<aside class="notes">
1st approach: Bag of Words: based on frequency, ignoring grammar or context. Also, vectors have 4k/8k dimensions depending on the training set.<br>
2nd approach: word2vec: shallow neural networks trained to reconstruct context. Vectors are distributed representations with a fixed dimension, independently on the training set.<br>
</aside>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin-left:auto;margin-right:auto}
.tg td{border-color:black;border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:34px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:34px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-18eh{border-color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-wp8o{border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-mqa1{border-color:#000000;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-18eh" rowspan="2">Model</th>
    <th class="tg-18eh" colspan="2">Median rank</th>
  </tr>
  <tr>
    <td class="tg-mqa1">Img. vs. txt</td>
    <td class="tg-mqa1">Txt vs. img.</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix">KCCA</td>
    <td class="tg-wp8o">52.42%</td>
    <td class="tg-wp8o">46.65%</td>
  </tr>
  <tr>
    <td class="tg-nrix">Ours-baseline</td>
    <td class="tg-wp8o">4.50%</td>
    <td class="tg-wp8o">4.54%</td>
  </tr>
  <tr>
    <td class="tg-nrix">Ours-word2vec</td>
    <td class="tg-mqa1">1.61%</td>
    <td class="tg-mqa1">1.63%</td>
  </tr>
</tbody>
</table>
<aside class="notes">
median position of the first correct result in the ranked list of results (in % of the total number of samples evaluated)<br>
Bag of Words: based on frequency, ignoring grammar or context. Also, vectors have 4k/8k dimensions depending on the training set.<br>
word2vec: shallow neural networks trained to reconstruct context. Vectors are distributed representations with a fixed dimension, independently on the training set.<br>
</aside>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Retrieval results</h2>
<table class="tg">
<thead>
  <tr>
    <th class="tg-18eh" rowspan="2">Model</th>
    <th class="tg-18eh" colspan="2">Image</th>
    <th class="tg-18eh" colspan="2">Text</th>
  </tr>
  <tr>
    <td class="tg-nrix"><span style="font-weight:700">f@5%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@10%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@5%</span></td>
    <td class="tg-nrix"><span style="font-weight:700">f@10%</span></td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-nrix">KCCA</td>
    <td class="tg-wp8o">3.70</td>
    <td class="tg-wp8o">7.59</td>
    <td class="tg-wp8o">3.90</td>
    <td class="tg-wp8o">9.59</td>
  </tr>
  <tr>
    <td class="tg-baqh">Ours-baseline</td>
    <td class="tg-wp8o">53.18</td>
    <td class="tg-wp8o">75.02</td>
    <td class="tg-wp8o">53.14</td>
    <td class="tg-wp8o">74.20</td>
  </tr>
  <tr>
    <td class="tg-baqh">Ours-word2vec</td>
    <td class="tg-mqa1">77.90</td>
    <td class="tg-mqa1">89.24</td>
    <td class="tg-mqa1">77.47</td>
    <td class="tg-mqa1">89.78</td>
  </tr>
</tbody>
</table>
</section>

<section data-background-image="imgs/backgrounds/blue-01.png">
<h2>Summary</h2>
<ul>
<li>Presented retrieval results on a <span class="fragment highlight-current-blue">challenging real-world dataset</span>, obtaining low median rank values</li>
<li>At the same time, <span class="fragment highlight-current-blue">very good classification accuracy</span> for both images and texts</li>
<li>Approach <span class="fragment highlight-current-blue">easily amenable</span> to large existing e-commerce datasets</li>
<li>Opportunity for e-commerces to <span class="fragment highlight-current-blue">classify and tag</span> images without associated text or fill in missing information looking for similar texts</li>
</ul>
<aside class="notes">
Training the embedding such that distances correspond to similarities, our approach can be easily used for retrieval tasks.<br>
Furthermore, our auxiliary classification networks encourage the embedding to have semantic meaning, making it suitable as features for classification tasks.<br>
</aside class="notes">
</section>